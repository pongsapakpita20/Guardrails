version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_storage:/root/.ollama
      # üëá 1. Mount ‡πÑ‡∏ü‡∏•‡πå script ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô container
      - ./ollama/entrypoint.sh:/entrypoint.sh
    # üëá 2. ‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô script ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
    # ‡∏™‡∏±‡πà‡∏á‡∏•‡∏ö \r ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡∏£‡∏±‡∏ô
    entrypoint: ["/bin/bash", "-c", "tr -d '\\r' < /entrypoint.sh | bash"]
    networks:
      - ai-net

  backend:
    build: 
      context: ./backend
    ports:
      - "8000:8000"
    volumes:
      - ./backend/src:/app/src
    environment:
      # ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ service 'ollama' ‡πÅ‡∏ó‡∏ô localhost ‡∏´‡∏£‡∏∑‡∏≠ IP
      - OLLAMA_URL=http://ollama:11434
      - GUARD_ENGINE=guardrails_ai
    depends_on:
      - ollama
    networks:
      - ai-net

  frontend:
    build:
      context: ./frontend
    ports:
      - "5173:5173"
    volumes:
      - ./frontend/src:/app/src
      - /app/node_modules
    environment:
      - VITE_API_URL=http://localhost:8000
    depends_on:
      - backend
    networks:
      - ai-net

volumes:
  ollama_storage:

networks:
  ai-net:
    driver: bridge